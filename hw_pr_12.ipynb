{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ec4a86-7179-4218-a831-b988f5a99584",
   "metadata": {},
   "source": [
    "Создание Spark сессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935edd3-eaca-4d08-9ae2-565c5dc2f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/su/spark-3.5.0-bin-hadoop3')\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "\n",
    "print(\"<<---***--- START ---***--->>\")\n",
    "\n",
    "spark = (SparkSession\n",
    " .builder\n",
    " .appName('pyspark_example')\n",
    " .enableHiveSupport()\n",
    " .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfac2541-1016-44bb-aab6-d9c3ee5bf23e",
   "metadata": {},
   "source": [
    "TASK 1\n",
    "Сгенерировать DataFrame из трёх колонок (row_id, discipline, season) - олимпийские \r\n",
    "дисциплины по сезонам. \n",
    " row_id - число порядкового номера строки;\r\n",
    " discipline - наименование олимпийский дисциплины на английском (полностью маленькии \r\n",
    "буквами; \r\n",
    " season - сезон дисциплины (summer / winer); \r\n",
    "*Укажите не менее чем по 5 дисциплин для каждого с\n",
    "зона. \r\n",
    "Сохраните DataFrame в csv-файл, разделитель колонок табуляция, первая строка должна \r\n",
    "содержать название колонок. \r\n",
    "Данные должны быть сохранены в виде 1 csv-файла а не множества маленьких."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f9971-4636-49d0-8560-addb44c776fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация DataFrame\n",
    "rows = [\n",
    "    (1, \"skiing\", \"winter\"),\n",
    "    (2, \"ice hockey\", \"winter\"),\n",
    "    (3, \"snowboarding\", \"winter\"),\n",
    "    (4, \"figure skating\", \"winter\"),\n",
    "    (5, \"biathlon\", \"winter\"),\n",
    "    (6, \"swimming\", \"summer\"),\n",
    "    (7, \"gymnastics\", \"summer\"),\n",
    "    (8, \"cycling\", \"summer\"),\n",
    "    (9, \"diving\", \"summer\"),\n",
    "    (10, \"beach volleyball\", \"summer\")\n",
    "]\n",
    "\n",
    "schema = \"row_id BIGINT, discipline STRING, season STRING\"\n",
    "df_ol = spark.createDataFrame(rows, schema)\n",
    "df_ol.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0496e2ad-6ff3-4376-a5cd-f0809012992e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Объединяем все данные в один RDD\n",
    "coalesced_df = df_ol.coalesce(1)\n",
    "# Сохраняем DataFrame в файл CSV\n",
    "coalesced_df.write \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"delimiter\", \"\\t\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .csv(\"data\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83414780-657f-4d09-b352-8c57ba5ca86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Читаем файл для проверки что все в порядке\n",
    "\n",
    "df_ol = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"delimiter\", \"\\t\")\n",
    "    .load(\"data/\")\n",
    ")\n",
    "\n",
    "# Вывод схемы DataFrame\n",
    "df_ol.printSchema()\n",
    "\n",
    "# Вывод первых строк DataFrame\n",
    "df_ol.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0402841-199a-490f-b821-138f3a58b24e",
   "metadata": {},
   "source": [
    "TASK 2\n",
    "Прочитайте исходный файл \"Athletes.csv\". \r\n",
    "Посчитайте в разрезе дисциплин сколько всего спортсменов в каждой из дисциплин \r\n",
    "принимало участие. \r\n",
    "Результат сохраните в формате parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4cc11e-228b-4dfa-8a0d-0557900e61e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение данных из CSV-файла в DataFrame\n",
    "df_atl = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"delimiter\", \";\")\n",
    "    .load(\"source/\")\n",
    ")\n",
    "\n",
    "# Вывод схемы DataFrame\n",
    "df_atl.printSchema()\n",
    "\n",
    "# Вывод первых 5 строк DataFrame\n",
    "df_atl.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454c149f-e664-4118-8968-4ce1749b865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# подсчет в разрезе дисциплин сколько всего спортсменов в каждой из дисциплин принимало участие\n",
    "# Создание временного представления для DataFrame\n",
    "df_atl.createOrReplaceTempView(\"athletes\")\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        Discipline,\n",
    "        COUNT(name) AS athletes\n",
    "    FROM athletes\n",
    "    GROUP BY Discipline;\n",
    "\"\"\")\n",
    "\n",
    "# Вывод первых 10 строк DataFrame с учетом длинных строк\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7f8ee9-bff9-44bf-bc09-a7eab2d4456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение результата в формате parquet\n",
    "# Запись DataFrame в формате Parquet с указанием сжатия GZIP и режима \"overwrite\"\n",
    "df.write.format(\"parquet\") \\\n",
    "        .option(\"compression\", \"gzip\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(\"data_out/\")\n",
    "\n",
    "# Чтение данных из Parquet-файла для проверки\n",
    "spark.read.format(\"parquet\") \\\n",
    "          .option(\"compression\", \"gzip\") \\\n",
    "          .load(\"data_out/\") \\\n",
    "          .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eded96eb-7476-44fc-80a9-1fd0c3cf50dd",
   "metadata": {},
   "source": [
    "TASK 3\n",
    "Прочитайте исходный файл \"Athletes.csv\". \r\n",
    "Посчитайте в разрезе дисциплин сколько всего спортсменов в каждой из дисциплин \r\n",
    "принимало участие. \r\n",
    "Получившийся результат нужно объединить с сгенерированным вами DataFrame из 1-го \r\n",
    "задания и в итоге вывести количество участников, только по тем дисциплинам, что есть в \r\n",
    "вашем сгенерированном DataFrame. \r\n",
    "Результат сохраните в формате paquet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8788be7-0cbe-45c4-a83d-9b70bdf52d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение исходного файла \"Athletes.csv\".\n",
    "df_atl = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"delimiter\", \";\")\n",
    "    .load(\"source/\")\n",
    ")\n",
    "\n",
    "df_ol = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"delimiter\", \"\\t\")\n",
    "    .load(\"data/\")\n",
    ")\n",
    "\n",
    "\n",
    "# Вывод схемы DataFrame\n",
    "df_atl.printSchema()\n",
    "df_ol.printSchema()\n",
    "\n",
    "# Вывод первых 5 строк DataFrame\n",
    "df_atl.show(5)\n",
    "df_ol.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2199c87f-d56d-4df9-a778-6b66857fd398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# подсчет в разрезе дисциплин сколько всего спортсменов в каждой из дисциплин принимало участие\n",
    "# Создание временного представления для DataFrame\n",
    "df_atl.createOrReplaceTempView(\"athletes\")\n",
    "df_ol.createOrReplaceTempView(\"olimpic\")\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        Discipline,\n",
    "        COUNT(name) AS athletes\n",
    "    FROM athletes\n",
    "    GROUP BY Discipline\n",
    "    HAVING Discipline in (SELECT INITCAP(discipline) FROM olimpic);\n",
    "\"\"\")\n",
    "\n",
    "# Вывод первых 10 строк DataFrame с учетом длинных строк\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6b323a-ad5e-42c3-95c7-804c1a13d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение результата в формате parquet\n",
    "# Запись DataFrame в формате Parquet с указанием сжатия GZIP и режима \"overwrite\"\n",
    "df.write.format(\"parquet\") \\\n",
    "        .option(\"compression\", \"gzip\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(\"result/\")\n",
    "\n",
    "# Чтение данных из Parquet-файла для проверки\n",
    "spark.read.format(\"parquet\") \\\n",
    "          .option(\"compression\", \"gzip\") \\\n",
    "          .load(\"result/\") \\\n",
    "          .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce0805e-4a28-4f3b-a7ce-fa6b368e2e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
